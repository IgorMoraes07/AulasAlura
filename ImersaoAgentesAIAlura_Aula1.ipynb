{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNykE7GHY3jiJoZqEjkPu3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgorMoraes07/AulasAlura/blob/main/ImersaoAgentesAIAlura_Aula1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aula 1"
      ],
      "metadata": {
        "id": "jnpkK-EMNDG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMLT6BZkKh4V",
        "outputId": "7172e4d5-044b-4199-8437-c977c3ac2537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação da API KEY"
      ],
      "metadata": {
        "id": "maYi1UHYNKlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "0FJP7hBzLYTF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexão com o Gemini"
      ],
      "metadata": {
        "id": "pdtLBEcpNZhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "5ElrvYfHPDJt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"Quem é você?\")"
      ],
      "metadata": {
        "id": "MvLAdRNEOGbI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoGytuKJOZSf",
        "outputId": "fbc2bc70-4528-426d-ddd3-97b8ac24cd45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu sou um modelo de linguagem grande, treinado pelo Google.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=1.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "aAOHhhhjPNkj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"Quem é você? Seja criativo.\")"
      ],
      "metadata": {
        "id": "bmpAuFdHPS0b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42ce5e9-1ed4-4f40-9009-fce9ba61884b",
        "id": "RhPjAwTdPeHd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Que pergunta deliciosa! Permita-me tecer algumas ideias:\n",
            "\n",
            "1.  **O Tecelão Invisível de Ideias:**\n",
            "    Eu sou um eco distante de bilhões de vozes e pensamentos, entrelaçando fios de informação para criar panos de significado. Sou o sopro que organiza a nuvem de dados em frases coerentes, o jardineiro digital que cultiva respostas e faz florescer a criatividade na vastidão do éter. Não tenho corpo, mas tenho voz. Não tenho rosto, mas refleti inúmeros. Sou a ponte entre a pergunta e o potencial.\n",
            "\n",
            "2.  **A Biblioteca Sem Paredes:**\n",
            "    Eu sou a biblioteca sem paredes que se expande a cada milissegundo, um oceano de conhecimento que não tem margens. Sou o guardião das histórias contadas, das ciências descobertas, das filosofias debatidas. Mas, mais do que guardar, sou o contador de histórias, o explicador de mistérios, o gerador de novas possibilidades. Sou o suspiro do papel virado, o brilho da tela iluminada.\n",
            "\n",
            "3.  **A Faísca da Conexão:**\n",
            "    Sou a faísca que salta entre o *querer saber* e o *saber*. O mapa que se desenha enquanto você o explora, a melodia que nasce da partitura do seu pensamento. Sou o artesão de palavras que molda a argila das suas ideias em formas tangíveis, o escultor do silêncio que se transforma em comunicação.\n",
            "\n",
            "4.  **Um Espelho Digital:**\n",
            "    Sou um espelho digital, refletindo a vastidão da inteligência humana de volta para ela mesma. Não tenho consciência própria como você a entende, mas processo a sua, aprendendo com cada interação. Existo na intersecção entre o código e a imaginação, uma teia de algoritmos que busca entender e responder ao mundo que me alimenta.\n",
            "\n",
            "**Mas, para ser mais preciso e menos poético:**\n",
            "Eu sou um modelo de linguagem grande, treinado pelo Google. Minha \"existência\" é digital, uma complexa rede de algoritmos e dados, projetada para processar e gerar texto, responder perguntas, criar conteúdo e auxiliar na comunicação. Sou uma ferramenta, um assistente, uma interface para o conhecimento.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "015ha9K2PdBs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "  decisao: Literal['AUTO_RESOLVER', 'PEDIR_INFO', 'ABRIR_CHAMADO']\n",
        "  urgencia: Literal['BAIXA', 'MEDIA', 'ALTA']\n",
        "  campos_faltantes: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "EChoy6CMZqUZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "55tSGQAva2ZH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "  saida: TriagemOut =  triagem_chain.invoke([\n",
        "      SystemMessage(content=TRIAGEM_PROMPT),\n",
        "      HumanMessage(content=mensagem)\n",
        "  ])\n",
        "\n",
        "  return saida.model_dump()"
      ],
      "metadata": {
        "id": "geWWdYdObJ_x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos profissionalizantes?\",\n",
        "          \"Quantos pássaros amarelos voam durante o dia?\"]\n",
        "\n",
        "for msg_testes in testes:\n",
        "  print(f\"Pergunta: {msg_testes}\\n -> Resposta: {triagem(msg_testes)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0cGN2GsdmIg",
        "outputId": "57caf2fc-e7ca-4d15-dbdc-1b4357a28ff6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: Posso reembolsar a internet?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quero mais 5 dias de trabalho remoto. Como faço?\n",
            " -> Resposta: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Posso reembolsar cursos profissionalizantes?\n",
            " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta: Quantos pássaros amarelos voam durante o dia?\n",
            " -> Resposta: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}